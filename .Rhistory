}
}
if(!exists_cluster) {
stopCluster(cl)
registerDoSEQ()
}
return(file.path(getwd(), 'Objects', name))
}
# n bootstrap hypervolumes for each size in seq.
bootstrap_seq <- function(name, hv, n = 10, points_per_resample = 'sample_size', seq = 3:nrow(hv@Data), cores = 1, verbose = TRUE) {
exists_cluster = TRUE
if(cores > 1 & getDoParWorkers() == 1) {
cl = makeCluster(cores)
clusterEvalQ(cl, {
library(hypervolume)
library(foreach)
source('Utils.R')
})
registerDoParallel(cl)
exists_cluster = FALSE
}
dir.create(file.path('./Objects', name))
foreach(i = seq) %do% {
subdir = paste('sample size', as.character(i))
dir.create(file.path('./Objects', name, subdir))
h = copy_param_hypervolume(hv, hv@Data[sample(1:nrow(hv@Data), i, replace = TRUE),], name = paste("resample", as.character(i)))
bootstrap(file.path(name, subdir), h, n, points_per_resample, verbose = verbose)
saveRDS(h, file.path('./Objects', name, subdir, 'original.rds'))
}
if(!exists_cluster) {
stopCluster(cl)
registerDoSEQ()
}
return(file.path(getwd(), 'Objects', name))
}
# Outputs k hypervolumes each with (k-1)/k of the total points used to create input hypervolume
k_split <- function(name, hv, k = 5, verbose = TRUE) {
dir.create(file.path('./Objects', name))
npoints = nrow(hv@Data)
shuffled = hv@Data[sample(1:npoints, npoints, replace = FALSE),]
if(verbose) {
pb = progress_bar$new(total = k*3)
}
foreach(i = 1:k, .combine = c) %do% {
range = (floor((i-1)*npoints/k) + 1):floor(i*npoints/k)
train_dat = shuffled[-1 * range,]
test_dat = shuffled[range,]
dir_name = paste('split', as.character(i))
dir.create(file.path('./Objects', name, dir_name))
h = copy_param_hypervolume(hv, data = train_dat, name = paste("train", as.character(i)))
path = paste0(h@Name, '.rds')
saveRDS(h, file.path('./Objects', name, dir_name, path))
if(verbose) {
pb$tick()
pb$tick()
}
t = copy_param_hypervolume(hv, data = test_dat, name = paste("test", as.character(i)))
path = paste0(t@Name, '.rds')
saveRDS(t, file.path('./Objects', name, dir_name, path))
if(verbose) {
pb$tick()
}
}
return(file.path(getwd(), 'Objects', name))
}
# Introduce bias using weights and multinomial random numbers
# Generate weights from distribution
# mu has same number of dimensions as cols_to_bias, point to bias towards
# sigma has same number of dimensions as cols_to_bias, strength of bias in each dimension
# Use mu and sigma as parameters of multivariate normal distribution or input own weight function that takes in a matrix argument
# cols_to_bias are indices of hv@Data
sampling_bias_bootstrap <- function(name, hv, n = 10, points_per_resample = 'sample_size', cores = 1, verbose = TRUE, mu = NULL, sigma = NULL, cols_to_bias = 1:ncol(hv@Data), weight_func = NULL) {
exists_cluster = TRUE
if(cores > 1 & getDoParWorkers() == 1) {
cl = makeCluster(cores)
clusterEvalQ(cl, {
library(hypervolume)
library(foreach)
library(mvtnorm)
source('Utils.R')
})
registerDoParallel(cl)
exists_cluster = FALSE
}
dir.create(file.path('./Objects', name))
if(verbose) {
pb = progress_bar$new(total = n)
}
foreach(i = 1:n, .combine = c) %dopar% {
if(is.null(weight_func)) {
if(length(mu) == 1) {
weights = dnorm(hv@Data[,cols_to_bias], mean = mu, sd = sqrt(sigma))
} else {
weights = dmvnorm(hv@Data[,cols_to_bias], mean = mu, sigma = diag(sigma))
}
} else {
weights = weight_func(hv@Data[,cols_to_bias])
}
if(points_per_resample == 'sample_size') {
points = apply(rmultinom(nrow(hv@Data), 1, weights) == 1, 2, which)
} else {
points = apply(rmultinom(points_per_resample, 1, weights) == 1, 2, which)
}
sample_dat = hv@Data[points,]
h = copy_param_hypervolume(hv, sample_dat, name = paste("resample", as.character(i)))
path = paste0(h@Name, '.rds')
saveRDS(h, file.path('./Objects', name, path))
if(verbose) {
pb$tick()
}
}
if(!exists_cluster) {
stopCluster(cl)
registerDoSEQ()
}
return(file.path(getwd(), 'Objects', name))
}
# Single interface for resampling
# Creates Objects directory in current working directory
# Returns absolute path to file with hypervolume files
resample <- function(name, hv, method, n = 10, points_per_resample = 'sample_size', seq = 3:nrow(hv@Data), cores = 1, verbose = TRUE, mu = NULL, sigma = NULL, cols_to_bias = 1:ncol(hv@Data)) {
if (n <= 0) {
stop("Invalid value for n")
} else if (points_per_resample != "sample size" & points_per_resample <= 0) {
stop("Invalid value for points_per_resample")
} else if (seq[1] <= 0) {
stop("Invalid input for seq")
} else if (method == 'biased bootstrap' & (length(mu) != length(sigma) | length(mu) != length(cols_to_bias))) {
stop("mu, sigma, and cols_to_bias must have same length")
} else if (cores < 1) {
stop("cores must be greater than or equal to 1")
}
dir.create('./Objects', showWarnings = FALSE)
if (method == 'bootstrap') {
return(bootstrap(name, hv, n, points_per_resample, cores, verbose))
} else if (method == 'bootstrap seq') {
return(bootstrap_seq(name, hv, n, points_per_resample, seq, cores, verbose))
} else if (method == 'biased bootstrap') {
return(sampling_bias_bootstrap(name, hv, n, points_per_resample, cores, verbose, mu, sigma, cols_to_bias))
}
}
detectCores()
detectCores(logical = F)
data("iris")
hypervolume(iris[,1:2])
hv <- hypervolume(iris[,1:2])
knitr::opts_chunk$set(echo = TRUE)
boot_path = resample("volume bias testing", hv, "bootstrap", n = 100)
cl = makeCluster(cores)
cores = 10
cl = makeCluster(cores)
clusterEvalQ(cl, {
library(hypervolume)
library(foreach)
source('Utils.R')
})
registerDoParallel(cl)
boot_path = resample("volume bias testing", hv, "bootstrap", n = 100)
full_data_mean_vol = 0
resample_data_mean_vol = 0
foreach(i = 1:100) %dopar% {
h = hypervolume(iris[,1:2])
full_data_mean_vol = full_data_mean_vol + get_volume(h)
}
foreach(i = list.files(boot_path)) %dopar% {
h = readRDS(file.path(boot_path, i))
resample_data_mean_vol = resample_data_mean_vol + get_volume(h)
}
full_data_mean_vol = 0
resample_data_mean_vol = 0
foreach(i = 1:100) %dopar% {
h = hypervolume(iris[,1:2])
full_data_mean_vol = full_data_mean_vol + get_volume(h)
}
foreach(i = list.files(boot_path)) %dopar% {
h = readRDS(file.path(boot_path, i))
resample_data_mean_vol = resample_data_mean_vol + get_volume(h)
}
get_volume(readRDS(file.path(boot_path, list.files(boot_path)[[1]])))
get_volume(readRDS(file.path(boot_path, list.files(boot_path)[[20]])))
get_volume(readRDS(file.path(boot_path, list.files(boot_path)[[90]])))
full_data_mean_vol = 0
resample_data_mean_vol = 0
full_vols = foreach(i = 1:100) %dopar% {
h = hypervolume(iris[,1:2])
get_volume(h)
}
boot_vols = foreach(i = list.files(boot_path)) %dopar% {
h = readRDS(file.path(boot_path, i))
get_volume(h)
}
full_data_mean_vol = 0
resample_data_mean_vol = 0
full_vols = foreach(i = 1:100, .combine = c) %dopar% {
h = hypervolume(iris[,1:2])
get_volume(h)
}
boot_vols = foreach(i = list.files(boot_path), .combine = c) %dopar% {
h = readRDS(file.path(boot_path, i))
get_volume(h)
}
full_data_mean_vol
full_vols
hist(boot_vols)
hist(full_vols, 30)
hist(full_vols, 20)
hist(full_vols, 20)
print(mean(boot_vols), mean(full_vols))
print(mean(boot_vols), mean(full_vols))
mean(boot_vols)
mean(full_vols)
t.test(full_vols, boot_vols)
help("t.test")
cores = 10
cl = makeCluster(cores)
clusterEvalQ(cl, {
library(hypervolume)
library(foreach)
source('Utils.R')
})
registerDoParallel(cl)
stopCluster(cl)
stopImplicitCluster()
t.test(full_vols, boot_vols)
reduced_boot_path = resample("volume bias testing", hv, "bootstrap", n = 100, points_per_resample = 100)
cores = 10
cl = makeCluster(cores)
clusterEvalQ(cl, {
library(hypervolume)
library(foreach)
source('Utils.R')
})
registerDoParallel(cl)
reduced_boot_path = resample("volume bias testing", hv, "bootstrap", n = 100, points_per_resample = 100)
reduced_boot_vols = foreach(i = list.files(reduced_boot_path), .combine = c) %dopar% {
h = readRDS(file.path(reduced_boot_path, i))
get_volume(h)
}
boot_path = resample("volume bias testing", hv, "bootstrap", n = 100)
full_vols = foreach(i = 1:100, .combine = c) %dopar% {
h = hypervolume(iris[,1:2])
get_volume(h)
}
boot_vols = foreach(i = list.files(boot_path), .combine = c) %dopar% {
h = readRDS(file.path(boot_path, i))
get_volume(h)
}
hist(full_vols, 20)
hist(full_vols, 20)
hist(boot_vols, 20)
hist(full_vols, 15)
hist(boot_vols, 15)
t.test(full_vols, boot_vols)
reduced_boot_path = resample("reduced points volume bias testing", hv, "bootstrap", n = 100, points_per_resample = 100)
reduced_boot_vols = foreach(i = list.files(reduced_boot_path), .combine = c) %dopar% {
h = readRDS(file.path(reduced_boot_path, i))
get_volume(h)
}
hist(reduced_boot_vols)
hist(reduced_boot_vols, 15)
hist(reduced_boot_vols, 15)
qqnorm(reduced_boot_vols)
hist(reduced_boot_vols, 15)
t.test(reduced_boot_vols, boot_vols)
reduced_boot_path = resample("reduced points volume bias testing", hv, "bootstrap", n = 150, points_per_resample = 100)
reduced_boot_vols = foreach(i = list.files(reduced_boot_path), .combine = c) %dopar% {
h = readRDS(file.path(reduced_boot_path, i))
get_volume(h)
}
hist(reduced_boot_vols, 15)
t.test(reduced_boot_vols, boot_vols)
stopCluster(cl)
stopImplicitCluster()
registerDoSEQ()
cores = 15
cl = makeCluster(cores)
clusterEvalQ(cl, {
library(hypervolume)
library(foreach)
source('Utils.R')
})
registerDoParallel(cl)
boot_path = resample("volume bias testing", hv, "bootstrap", n = 100)
full_vols = foreach(i = 1:100, .combine = c) %dopar% {
h = hypervolume(iris[,1:2])
get_volume(h)
}
boot_vols = foreach(i = list.files(boot_path), .combine = c) %dopar% {
h = readRDS(file.path(boot_path, i))
get_volume(h)
}
hist(full_vols, 15)
hist(boot_vols, 15)
t.test(full_vols, boot_vols)
reduced_boot_path = resample("reduced points volume bias testing", hv, "bootstrap", n = 150, points_per_resample = 100)
reduced_boot_vols = foreach(i = list.files(reduced_boot_path), .combine = c) %dopar% {
h = readRDS(file.path(reduced_boot_path, i))
get_volume(h)
}
hist(reduced_boot_vols, 15)
t.test(reduced_boot_vols, boot_vols)
stopCluster(cl)
stopImplicitCluster()
registerDoSEQ()
cores = 20
cl = makeCluster(cores)
clusterEvalQ(cl, {
library(hypervolume)
library(foreach)
source('Utils.R')
})
registerDoParallel(cl)
boot_path = resample("volume bias testing", hv, "bootstrap", n = 500)
full_vols = foreach(i = 1:500, .combine = c) %dopar% {
h = hypervolume(iris[,1:2])
get_volume(h)
}
boot_vols = foreach(i = list.files(boot_path), .combine = c) %dopar% {
h = readRDS(file.path(boot_path, i))
get_volume(h)
}
hist(full_vols, 15)
hist(boot_vols, 15)
t.test(full_vols, boot_vols)
reduced_boot_path = resample("reduced points volume bias testing", hv, "bootstrap", n = 500, points_per_resample = 100)
reduced_boot_vols = foreach(i = list.files(reduced_boot_path), .combine = c) %dopar% {
h = readRDS(file.path(reduced_boot_path, i))
get_volume(h)
}
hist(reduced_boot_vols, 15)
t.test(reduced_boot_vols, boot_vols)
nrow(hv@Data)
manual_boot_path = resample("volume bias testing", hv, "bootstrap", n = 500, points_per_resample = 150)
manual_boot_vols = foreach(i = list.files(manual_boot_path), .combine = c) %dopar% {
h = readRDS(file.path(manual_boot_path, i))
get_volume(h)
}
t.test(manual_boot_vols, full_vols)
t.test(manual_boot_vols, boot_vols)
knitr::opts_chunk$set(echo = TRUE)
source('resample.R')
source('Funnels.R')
install.packages("dplyr")
source('resample.R')
source('Funnels.R')
install.packages("ggplot2")
source('resample.R')
source('Funnels.R')
source('Utils.R')
data(iris)
data(quercus)
hv = hypervolume(iris[,c(1, 2)])
# Samples i rows from the data that generated hv for each i in seq, then generates a bootstrap on each of these i resamples and saves to file.
# Runtime ~2hrs
# hv_bootstrap_seq <- resample('hv_bootstrap_seq', hv, method = 'bootstrap seq', points_per_resample = "sample_size", seq = seq(10, 150, length.out = 29))
# Here is the result of running the above code
hv_bootstrap_seq <- "./Objects/hv_bootstrap_seq"
funnel(hv_bootstrap_seq, title = 'From resampled data', func = function(x) {mean(x@Data[,1])}) +
geom_line(aes(y = mean(hv@Data[,1]))) +
ylim(5.25, 6.5) +
ylab('Mean Sepal Length')
funnel(hv_bootstrap_seq, title = 'From random points', func = function(x) {mean(x@RandomPoints[,1])}) +
geom_line(aes(y = mean(hv@Data[,1]))) +
ylim(5.25, 6.5) +
ylab('Mean Sepal Length')
funnel(hv_bootstrap_seq, title = 'Resampled variances from random points', func = function(x) {var(x@RandomPoints[,1])}) +
geom_line(aes(y = var(hv@Data[,1])))
funnel(hv_bootstrap_seq, title = 'Resampled volumes', func = get_volume) +
geom_line(aes(y = get_volume(hv)))
# Hypervolume generated from iris data
hv = hypervolume(iris[,1:4])
# Weigh points with large values for petal length and petal width higher
biased_path = resample("Petal bias", hv, method = "biased bootstrap", n = 1, mu = apply(hv@Data, 2, max)[c("Petal.Length", "Petal.Width")], sigma = apply(hv@Data, 2, var)[c("Petal.Length", "Petal.Width")]*2, cols_to_bias = c("Petal.Length", "Petal.Width"))
stopCluster(cl)
stopImplicitCluster()
registerDoSEQ()
# Hypervolume generated from iris data
hv = hypervolume(iris[,1:4])
# Weigh points with large values for petal length and petal width higher
biased_path = resample("Petal bias", hv, method = "biased bootstrap", n = 1, mu = apply(hv@Data, 2, max)[c("Petal.Length", "Petal.Width")], sigma = apply(hv@Data, 2, var)[c("Petal.Length", "Petal.Width")]*2, cols_to_bias = c("Petal.Length", "Petal.Width"))
biased_hv = readRDS(file.path(biased_path, "resample 1.rds"))
# Combine resulting data in a dataframe
dat = data.frame(rbind(hv@Data, biased_hv@Data))
dat['Type'] = rep(c('original', 'biased'), each = 150)
ggplot(dat, aes(y = ..density..)) + geom_histogram(aes(x = Petal.Width, fill = Type), bins = 20) +
facet_wrap(~Type) +
ggtitle("Distribution of Petal Width", "Biased resample vs Original sample")
ggplot(dat, aes(y = ..density..)) + geom_histogram(aes(x = Petal.Length, fill = Type), bins = 20) +
facet_wrap(~Type) +
ggtitle("Distribution of Petal Length", "Biased resample vs Original sample")
ggplot(dat, aes(y = ..density..)) + geom_histogram(aes(x = Sepal.Width, fill = Type), bins = 20) +
facet_wrap(~Type) +
ggtitle("Distribution of Sepal Width", "Biased resample vs Original sample")
ggplot(dat, aes(y = ..density..)) + geom_histogram(aes(x = Sepal.Length, fill = Type), bins = 20) +
facet_wrap(~Type) +
ggtitle("Distribution of Sepal Length", "Biased resample vs Original sample")
hv_quercus = hypervolume(quercus[,c(2,3)])
# Runtime ~6hrs
# quercus_bootstrap_seq <- resample('quercus_bootstrap_seq', hv_quercus, method = 'bootstrap seq', points_per_resample = "sample_size", seq = floor(seq(10, 3779, length.out = 30)))
quercus_bootstrap_seq <- "./Objects/quercus_bootstrap_seq"
funnel(quercus_bootstrap_seq, title = 'Resampled volumes of Quercus', func = get_volume) +
geom_line(aes(y = get_volume(hv_quercus)))
# bias towards mean latitude
mu = mean(hv_quercus@Data[,2])
sigma = 3*var(hv_quercus@Data[,2])
# Biased bootstrap on hv_quercus
# Runtime ~ 20min
# center_bias = resample('quercus bias', hv_quercus, method = 'biased bootstrap', mu = mu, sigma = sigma, cols_to_bias = 2)
biased_quercus_hvs = to_hv_list("./Objects/quercus bias")
weight_func = function(x) {dnorm(x, mu, sqrt(sigma))}
ggplot(quercus, aes(x = Longitude)) + stat_function(fun = weight_func) +
geom_rug() +
ggtitle('Weights biased towards mean longitude') +
ylab('resampling weights')
ggplot(data.frame(volume = get_volume(biased_quercus_hvs)), aes(x = volume)) +
geom_histogram(bins = 5) +
ggtitle('Volumes of hypervolumes biased towards mean longitude', 'Original volume: 106.8')
plot(hv)
getDoParWorkers()
model_bootstrap <- function(name, hv, n = 10, points_per_resample = 'sample_size', cores = 1, verbose = TRUE) {
exists_cluster = TRUE
if(cores > 1 & getDoParWorkers() == 1) {
cl = makeCluster(cores)
clusterEvalQ(cl, {
library(hypervolume)
library(foreach)
source('Utils.R')
})
registerDoParallel(cl)
exists_cluster = FALSE
}
dir.create(file.path('./Objects', name))
if(verbose) {
pb = progress_bar$new(total = n)
}
foreach(i = 1:n, .combine = c) %dopar% {
if(points_per_resample == 'sample_size') {
sample_dat = hv@RandomPoints[sample(1:nrow(hv@RandomPoints), nrow(hv@Data), replace = TRUE),]
} else {
sample_dat = hv@RandomPoints[sample(1:nrow(hv@RandomPoints), points_per_resample, replace = TRUE),]
}
h = copy_param_hypervolume(hv, sample_dat, name = paste("resample", as.character(i)))
path = paste0(h@Name, '.rds')
saveRDS(h, file.path('./Objects', name, path))
if(verbose) {
pb$tick()
}
}
if(!exists_cluster) {
stopCluster(cl)
registerDoSEQ()
}
return(file.path(getwd(), 'Objects', name))
}
hv = hypervolume(iris[,1:2])
cores = 20
cl = makeCluster(cores)
clusterEvalQ(cl, {
library(hypervolume)
library(foreach)
source('Utils.R')
})
registerDoParallel(cl)
boot_path = resample("volume bias testing", hv, "bootstrap", n = 500)
full_vols = foreach(i = 1:500, .combine = c) %dopar% {
h = hypervolume(iris[,1:2])
get_volume(h)
}
boot_vols = foreach(i = list.files(boot_path), .combine = c) %dopar% {
h = readRDS(file.path(boot_path, i))
get_volume(h)
}
hist(full_vols, 15)
hist(boot_vols, 15)
t.test(full_vols, boot_vols)
reduced_boot_path = resample("reduced points volume bias testing", hv, "bootstrap", n = 500, points_per_resample = 100)
reduced_boot_vols = foreach(i = list.files(reduced_boot_path), .combine = c) %dopar% {
h = readRDS(file.path(reduced_boot_path, i))
get_volume(h)
}
hist(reduced_boot_vols, 15)
t.test(reduced_boot_vols, boot_vols)
manual_boot_path = resample("manual volume bias testing", hv, "bootstrap", n = 500, points_per_resample = 150)
manual_boot_vols = foreach(i = list.files(manual_boot_path), .combine = c) %dopar% {
h = readRDS(file.path(manual_boot_path, i))
get_volume(h)
}
t.test(manual_boot_vols, boot_vols)
manual_boot_path = resample("manual volume bias testing", hv, "bootstrap", n = 500, points_per_resample = 150)
manual_boot_vols = foreach(i = list.files(manual_boot_path), .combine = c) %dopar% {
h = readRDS(file.path(manual_boot_path, i))
get_volume(h)
}
t.test(manual_boot_vols, boot_vols)
model_path = model_bootstrap("resample from random points", hv, n = 500, cores)
model_path = model_bootstrap("resample from random points", hv, n = 500)
model_vols = foreach(i = list.files(model_path), .combine = c) %dopar% {
h = readRDS(file.path(model_path, i))
get_volume(h)
}
t.test(full_vols, model_vols)
get_volume(hv)
help("hypervolume")
alt_full_vols = foreach(i = 1:500, .combine = c) %dopar% {
h = hypervolume(iris[,1:2], kde.bandwidth = estimate_bandwidth(iris[,1:2], "cross-validation"))
get_volume(h)
}
t.test(alt_full_vols, full_vols)
plot(hypervolume(iris[,1:2], kde.bandwidth = estimate_bandwidth(iris[,1:2], "cross-validation")))
plot(hv)
var(full_vols)
hv
